{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSWEcS2XKgzi"
      },
      "source": [
        "### Parameter Efficient Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Original library versions\n",
        "# %pip install --quiet transformers==4.34.1 accelerate==0.24.0 sentencepiece==0.1.99 optimum==1.13.2 peft==0.5.0 bitsandbytes==0.41.2.post2\n",
        "\n",
        "# Preferred versions for Colab as of October 2025 (thanks, Lev!)\n",
        "# %pip install --quiet \"bitsandbytes==0.45.3\" \"transformers>=4.43,<4.46\" \"accelerate>=0.33,<0.36\" \"peft>=0.11.1\" \"optimum>=1.20.0\" \"sentencepiece\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xeRF_hSKgzs",
        "outputId": "6b3524b2-a1c0-4a67-c4ec-c22b01757eea"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import transformers\n",
        "from tqdm.auto import tqdm, trange\n",
        "\n",
        "assert torch.cuda.is_available(), \"you need cuda for this part\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "8afd16e7ce954ef3a8904bd5ab58c58b",
            "6423f3812d6646c6a416765f7710ebff",
            "cf077c6f0f2347faad3ab340608017a4",
            "7b45cca01b224a598961f4b281c41418",
            "71f8dcf1e6f2421d8d0e0aa57e2880ac",
            "5d40abb9d0514fccbea444fc6d2326d3",
            "93955e8872df4a85ab7db725856546d2",
            "c6ed8fa489d54cf7bd83c8ec1890242b",
            "32d98d7256654cd58730286d1e93c6fe",
            "5e210039d1ac4c7f9ef85f65cd5b0371",
            "11019f88b11f4be39a22aada133b0a6e"
          ]
        },
        "id": "VMzFwx29Kgzu",
        "outputId": "19fbe94a-1c12-41c8-8ea8-ad556095a8b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93e9e8dde6374727868258e226d3aebb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name = 'Enoch/llama-7b-hf'\n",
        "\n",
        "# loading Llama tokenizer ...\n",
        "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# ... and the model itself\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map='auto',\n",
        "    low_cpu_mem_usage=True,\n",
        "    offload_state_dict=True,\n",
        "    load_in_4bit=True,\n",
        "    torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
        ")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "\n",
        "model.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\n",
        "model.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n",
        "# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgspB2JwSIS2"
      },
      "source": [
        "### Prompt tuning: the story of a fox \n",
        "\n",
        "![img](https://i.imgur.com/Ux3qQAu.png) (source: theodd1souts.fandom.com)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H13pYFRxQi4U",
        "outputId": "eebf630a-5321-4965-b4ee-0251a2b22d8b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Output: <s>A quick brown fox jumps over the lazy dog.\n",
            "A quick\n"
          ]
        }
      ],
      "source": [
        "prompt = 'A quick brown fox'\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "\n",
        "for i in range(10):\n",
        "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r6UVDl4NEua",
        "outputId": "81f8adeb-fc41-4abb-d05b-73c69d08cafb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: tensor(3.0729, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "outputs = model(**batch)\n",
        "\n",
        "next_word_logits = outputs.logits[:, :-1]\n",
        "true_next_tokens = batch['input_ids'][:, 1:]\n",
        "loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "\n",
        "print(\"Loss:\", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amvNufS8WXa0"
      },
      "source": [
        "Except, we can't train the entire model - that would be 28GB gradients in float32. Instead, let's run [prompt tuning](https://arxiv.org/abs/2104.08691).\n",
        "\n",
        "![img](https://i.imgur.com/VwNNKnb.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "73ZOCFRZWR98"
      },
      "outputs": [],
      "source": [
        "class WordEmbeddingsWithLearnedPrompts(nn.Module):\n",
        "    \"\"\"\n",
        "    To perform prompt tuning, you will need to replace the model's original word embeddings with a layer - THIS layer\n",
        "    - that inserts trainable prompts instead of the first N token embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, word_embeddings: nn.Embedding, num_prompts: int):\n",
        "        super().__init__()\n",
        "        self.original_word_embeddings = word_embeddings\n",
        "        self.num_prompts = num_prompts\n",
        "        self.learnable_prompts = nn.Parameter(\n",
        "            torch.randn(1, num_prompts, word_embeddings.embedding_dim), requires_grad=True\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids: torch.LongTensor):\n",
        "        # input_ids shape: [batch_size, seq_length]\n",
        "        assert input_ids.dtype == torch.int64\n",
        "        assert input_ids.shape[1] > self.num_prompts\n",
        "        assert torch.all(input_ids[:, :self.num_prompts] == tokenizer.pad_token_id).item(), (\n",
        "            \"Don't forget to prepend several BOS tokens to input_ids\"\n",
        "        )\n",
        "\n",
        "        # Embed the input_ids using the original word embeddings\n",
        "        input_embeddings = self.original_word_embeddings(input_ids)  # Shape: [batch_size, seq_length, embedding_dim]\n",
        "\n",
        "        # Replace the first num_prompts token embeddings with the learnable prompts\n",
        "        batch_size = input_ids.shape[0]\n",
        "        learnable_prompts_expanded = self.learnable_prompts.expand(batch_size, -1, -1)  # Shape: [batch_size, num_prompts, embedding_dim]\n",
        "        remaining_embeddings = input_embeddings[:, self.num_prompts:, :]  # Shape: [batch_size, seq_length - num_prompts, embedding_dim]\n",
        "\n",
        "        # Concatenate learnable prompts with the embeddings of the remaining tokens\n",
        "        output_embeddings = torch.cat([learnable_prompts_expanded, remaining_embeddings], dim=1)\n",
        "\n",
        "        return output_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxUyUU2uT2f1",
        "outputId": "7e0bcfb6-ebc1-4cad-8dc8-7da2ef73a88b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looks legit!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_5020/1152930240.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        }
      ],
      "source": [
        "num_prompts = 16\n",
        "test_emb_layer = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n",
        "test_input_ids = tokenizer(\"a cat say on a may\", return_tensors='pt')['input_ids'].to(device)\n",
        "\n",
        "space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n",
        "                               dtype=torch.int64, device=device)\n",
        "test_inputs_with_prompts = torch.cat([space_for_prompts, test_input_ids], dim=1)\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "  test_prompt_embeddings = test_emb_layer(test_inputs_with_prompts)\n",
        "\n",
        "assert test_prompt_embeddings.shape[:2] == test_inputs_with_prompts.shape\n",
        "assert test_prompt_embeddings.shape[-1] == model.config.hidden_size\n",
        "assert torch.allclose(test_prompt_embeddings[:, :num_prompts], test_emb_layer.learnable_prompts.float())\n",
        "assert torch.allclose(test_prompt_embeddings[:, num_prompts:], model.model.embed_tokens(test_input_ids).float())\n",
        "print(\"Looks legit!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QRe0lpREV49G"
      },
      "outputs": [],
      "source": [
        "assert isinstance(model.model.embed_tokens, nn.Embedding), \"you have already replaced the embedding layer. If the replacement is broken, please reload the model\"\n",
        "\n",
        "model.model.embed_tokens = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n",
        "\n",
        "opt = torch.optim.Adam([model.model.embed_tokens.learnable_prompts], lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gVQzgdka-Bm",
        "outputId": "2a56c19f-0027-494a-b1c3-7d961e8b103f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(7.9402, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch 0, Loss: 7.9402\n",
            "Epoch 20, Loss: 2.3153\n",
            "Epoch 40, Loss: 0.2475\n",
            "Epoch 60, Loss: 0.0310\n",
            "Epoch 80, Loss: 0.0127\n"
          ]
        }
      ],
      "source": [
        "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n",
        "                               dtype=torch.int64, device=device)\n",
        "batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n",
        "batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n",
        "\n",
        "outputs = model(**batch)\n",
        "next_word_logits = outputs.logits[:, num_prompts : -1, :]\n",
        "true_next_tokens = batch['input_ids'][:, num_prompts + 1:]\n",
        "loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "print(\"Loss:\", loss)\n",
        "\n",
        "\n",
        "# raise NotImplementedError(\"Your task: iteratively train the model to reduce loss using prompt optimizer (opt)\")\n",
        "\n",
        "model.train()\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    opt.zero_grad()\n",
        "\n",
        "    outputs = model(**batch)\n",
        "    next_word_logits = outputs.logits[:, num_prompts : -1, :]\n",
        "    true_next_tokens = batch['input_ids'][:, num_prompts + 1:]\n",
        "    loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0UmHFH68JnD",
        "outputId": "bc4621b5-7911-494f-949c-391980fa0070"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good job!\n"
          ]
        }
      ],
      "source": [
        "# Final loss assertion\n",
        "assert loss.item() <= 0.1\n",
        "print(\"Good job!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7DkWHD-r1Xo",
        "outputId": "5ef91a6d-964d-4e1d-8401-cc96ab0df0ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Output: <s>A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it\n"
          ]
        }
      ],
      "source": [
        "prompt = 'A quick brown fox'\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n",
        "batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n",
        "\n",
        "\n",
        "for i in range(15):\n",
        "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0, num_prompts:].cpu().numpy().tolist()))\n",
        "\n",
        "# if you did everything right, the model will deny that the fox jumped over the lazy dog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEkoFNdlshv_"
      },
      "source": [
        "### Using HuggingFace PEFT \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,  \n",
        "    bnb_4bit_quant_type=\"nf4\",             \n",
        "    bnb_4bit_use_double_quant=True,        \n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map={\"\": 0},\n",
        "    low_cpu_mem_usage=True,\n",
        "    quantization_config=quantization_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqEEpZm2Q4UC",
        "outputId": "dc53e7e1-af92-488b-850d-2e94c9f682a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable parameters: 65536\n",
            "Total parameters (excluding quantization): 3500478464\n"
          ]
        }
      ],
      "source": [
        "import peft\n",
        "assert isinstance(model.model.embed_tokens, nn.Embedding), \"please reload the model\"\n",
        "\n",
        "peft_config = peft.PromptTuningConfig(task_type=peft.TaskType.CAUSAL_LM, num_virtual_tokens=16)\n",
        "model = peft.get_peft_model(model, peft_config)  # note: for most peft methods, this line also modifies model in-place\n",
        "print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "print(\"Total parameters (excluding quantization):\", sum(p.numel() for p in model.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xSc3oVSW-ujh"
      },
      "outputs": [],
      "source": [
        "# Define the ground truth sentence\n",
        "# the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "batch = tokenizer(the_truth, return_tensors=\"pt\", return_token_type_ids=False).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_ac7RxO-xYg",
        "outputId": "ad5706dd-ef6e-413b-fe25-cf80ded313af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Loss: 7.7810378074646\n",
            "Epoch 2/100, Loss: 7.4751691818237305\n",
            "Epoch 3/100, Loss: 7.222239971160889\n",
            "Epoch 4/100, Loss: 7.017329692840576\n",
            "Epoch 5/100, Loss: 6.850759506225586\n",
            "Epoch 6/100, Loss: 6.703004360198975\n",
            "Epoch 7/100, Loss: 6.572248935699463\n",
            "Epoch 8/100, Loss: 6.4522318840026855\n",
            "Epoch 9/100, Loss: 6.337976455688477\n",
            "Epoch 10/100, Loss: 6.224721908569336\n",
            "Epoch 11/100, Loss: 6.1108293533325195\n",
            "Epoch 12/100, Loss: 5.9946818351745605\n",
            "Epoch 13/100, Loss: 5.8746418952941895\n",
            "Epoch 14/100, Loss: 5.755471229553223\n",
            "Epoch 15/100, Loss: 5.641081809997559\n",
            "Epoch 16/100, Loss: 5.527312755584717\n",
            "Epoch 17/100, Loss: 5.416666030883789\n",
            "Epoch 18/100, Loss: 5.302328109741211\n",
            "Epoch 19/100, Loss: 5.1896071434021\n",
            "Epoch 20/100, Loss: 5.080301761627197\n",
            "Epoch 21/100, Loss: 4.97166633605957\n",
            "Epoch 22/100, Loss: 4.864184856414795\n",
            "Epoch 23/100, Loss: 4.765018463134766\n",
            "Epoch 24/100, Loss: 4.665083408355713\n",
            "Epoch 25/100, Loss: 4.564681529998779\n",
            "Epoch 26/100, Loss: 4.464593887329102\n",
            "Epoch 27/100, Loss: 4.365787029266357\n",
            "Epoch 28/100, Loss: 4.263604164123535\n",
            "Epoch 29/100, Loss: 4.1610260009765625\n",
            "Epoch 30/100, Loss: 4.059431076049805\n",
            "Epoch 31/100, Loss: 3.957507371902466\n",
            "Epoch 32/100, Loss: 3.855135202407837\n",
            "Epoch 33/100, Loss: 3.750368356704712\n",
            "Epoch 34/100, Loss: 3.6479532718658447\n",
            "Epoch 35/100, Loss: 3.545186758041382\n",
            "Epoch 36/100, Loss: 3.4416704177856445\n",
            "Epoch 37/100, Loss: 3.337299108505249\n",
            "Epoch 38/100, Loss: 3.235244035720825\n",
            "Epoch 39/100, Loss: 3.1343469619750977\n",
            "Epoch 40/100, Loss: 3.0332183837890625\n",
            "Epoch 41/100, Loss: 2.9308645725250244\n",
            "Epoch 42/100, Loss: 2.8319292068481445\n",
            "Epoch 43/100, Loss: 2.732825756072998\n",
            "Epoch 44/100, Loss: 2.635848045349121\n",
            "Epoch 45/100, Loss: 2.5397279262542725\n",
            "Epoch 46/100, Loss: 2.4470643997192383\n",
            "Epoch 47/100, Loss: 2.3546228408813477\n",
            "Epoch 48/100, Loss: 2.2672698497772217\n",
            "Epoch 49/100, Loss: 2.1807100772857666\n",
            "Epoch 50/100, Loss: 2.0948660373687744\n",
            "Epoch 51/100, Loss: 2.0118560791015625\n",
            "Epoch 52/100, Loss: 1.930435061454773\n",
            "Epoch 53/100, Loss: 1.8517920970916748\n",
            "Epoch 54/100, Loss: 1.7721062898635864\n",
            "Epoch 55/100, Loss: 1.6949286460876465\n",
            "Epoch 56/100, Loss: 1.614833950996399\n",
            "Epoch 57/100, Loss: 1.5374993085861206\n",
            "Epoch 58/100, Loss: 1.4616940021514893\n",
            "Epoch 59/100, Loss: 1.3840872049331665\n",
            "Epoch 60/100, Loss: 1.3098621368408203\n",
            "Epoch 61/100, Loss: 1.2348897457122803\n",
            "Epoch 62/100, Loss: 1.1609195470809937\n",
            "Epoch 63/100, Loss: 1.0873509645462036\n",
            "Epoch 64/100, Loss: 1.0146665573120117\n",
            "Epoch 65/100, Loss: 0.9457080364227295\n",
            "Epoch 66/100, Loss: 0.8761565685272217\n",
            "Epoch 67/100, Loss: 0.8112695217132568\n",
            "Epoch 68/100, Loss: 0.7480217218399048\n",
            "Epoch 69/100, Loss: 0.6902212500572205\n",
            "Epoch 70/100, Loss: 0.6333699822425842\n",
            "Epoch 71/100, Loss: 0.5812437534332275\n",
            "Epoch 72/100, Loss: 0.534025251865387\n",
            "Epoch 73/100, Loss: 0.48888662457466125\n",
            "Epoch 74/100, Loss: 0.4467436969280243\n",
            "Epoch 75/100, Loss: 0.40890976786613464\n",
            "Epoch 76/100, Loss: 0.3724730610847473\n",
            "Epoch 77/100, Loss: 0.3393220901489258\n",
            "Epoch 78/100, Loss: 0.307686448097229\n",
            "Epoch 79/100, Loss: 0.2791091799736023\n",
            "Epoch 80/100, Loss: 0.252463161945343\n",
            "Epoch 81/100, Loss: 0.22863830626010895\n",
            "Epoch 82/100, Loss: 0.20665282011032104\n",
            "Epoch 83/100, Loss: 0.1882026344537735\n",
            "Epoch 84/100, Loss: 0.1701347976922989\n",
            "Epoch 85/100, Loss: 0.15491493046283722\n",
            "Epoch 86/100, Loss: 0.14164942502975464\n",
            "Epoch 87/100, Loss: 0.12909220159053802\n",
            "Epoch 88/100, Loss: 0.11854717135429382\n",
            "Epoch 89/100, Loss: 0.10841639339923859\n",
            "Epoch 90/100, Loss: 0.10067982971668243\n",
            "Epoch 91/100, Loss: 0.09236953407526016\n",
            "Loss threshold reached. Stopping training.\n"
          ]
        }
      ],
      "source": [
        "# Training Configuration\n",
        "loss_threshold = 0.1  # Desired loss threshold\n",
        "num_epochs = 100  # Max number of epochs\n",
        "learning_rate = 3e-3  # Learning rate\n",
        "\n",
        "# Define the optimizer for trainable parameters (PEFT prompts)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),       # PEFT автоматически помечает только новые параметры как requires_grad=True\n",
        "    lr=learning_rate,\n",
        "    weight_decay=0.01         \n",
        ")\n",
        "\n",
        "# градиентный клиппинг\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "# Define the ground truth\n",
        "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "batch = tokenizer(the_truth, return_tensors=\"pt\", return_token_type_ids=False).to(device)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    num_virtual = model.peft_config[\"default\"].num_virtual_tokens\n",
        "\n",
        "    # Forward pass\n",
        "    outputs =  model(**batch)\n",
        "\n",
        "    # Skip logits for virtual tokens and the last token\n",
        "    next_word_logits = outputs.logits[:, num_virtual:-1] # Skip virtual tokens\n",
        "    true_next_tokens = batch[\"input_ids\"][:, 1:]  # Shift ground truth tokens by one\n",
        "\n",
        "    # Сделаем проверку\n",
        "    assert next_word_logits.shape[1] == true_next_tokens.shape[1], \\\n",
        "    f\"Shape mismatch: logits {next_word_logits.shape} vs targets {true_next_tokens.shape}\"\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = F.cross_entropy(\n",
        "        next_word_logits.reshape(-1, next_word_logits.size(-1)),  \n",
        "        true_next_tokens.reshape(-1)\n",
        "    )\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()  \n",
        "    loss.backward()        \n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) #клиппинг\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print loss for tracking\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "    # Stop training if loss is below threshold\n",
        "    if loss.item() < loss_threshold:\n",
        "        print(\"Loss threshold reached. Stopping training.\")\n",
        "        break\n",
        "else:\n",
        "    print(\"Maximum epochs reached without meeting the loss threshold.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyyfLYRY-12Z",
        "outputId": "51579116-9e5c-48be-85ef-74af241da867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training successful! Loss is below 0.1.\n"
          ]
        }
      ],
      "source": [
        "# Final assertion to ensure loss is below threshold\n",
        "assert loss.item() < loss_threshold, \"Training failed to reduce loss below threshold.\"\n",
        "print(\"Training successful! Loss is below 0.1.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiQbJ3Wt-25Z",
        "outputId": "4c423987-acb5-4fd4-a281-665ce0d5ef0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Output: A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it\n"
          ]
        }
      ],
      "source": [
        "prompt = \"A quick brown fox\"\n",
        "batch = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(device)\n",
        "\n",
        "# Generate 18 tokens\n",
        "for i in range(15):\n",
        "    # Forward pass to get the logits\n",
        "    outputs = model(**batch)\n",
        "    next_token = outputs.logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "\n",
        "    # Append the next token to input_ids\n",
        "    batch[\"input_ids\"] = torch.cat([batch[\"input_ids\"], next_token], dim=-1)\n",
        "\n",
        "    # Update the attention_mask to match the new input_ids length\n",
        "    new_attention_mask = torch.ones_like(next_token, dtype=batch[\"attention_mask\"].dtype).to(device)\n",
        "    batch[\"attention_mask\"] = torch.cat([batch[\"attention_mask\"], new_attention_mask], dim=-1)\n",
        "\n",
        "# Decode the generated sequence\n",
        "# Skip the virtual tokens (if applicable) by slicing `batch[\"input_ids\"][:, num_prompts:]`\n",
        "decoded_output = tokenizer.decode(batch[\"input_ids\"][0].cpu().numpy().tolist(), skip_special_tokens=True)\n",
        "print(\"\\nOutput:\", decoded_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCkpKYjWxfhk"
      },
      "source": [
        "### Parameter-efficient finetuning with LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b87c54c3bed847ab933eae8175359169",
            "8aaa22971b3e416eae8394ef3b3b3f0f",
            "e9c4ba0d262c4b76baa00532e209b92f",
            "e6f9064e6ec545debe2e90163f4c712c",
            "6aad5b046def4a7db1048434e874b5d5",
            "dba48e929a2e43ec8e4bb3d4b32475ca",
            "530d66e4732b4d5486165654415bd2dc",
            "2bd4b6acd8004c1e98c064708108938e",
            "09b07105e4f54d2bb5e6cc1c1f1a9c8e",
            "923869a5864c4d3d80fb76c99fff24e2",
            "25e1f3d72230485c9b84cae4f685a69a"
          ]
        },
        "id": "8zundaSzx90r",
        "outputId": "3faf7150-7685-4089-cf58-e03e4fce8bc6"
      },
      "outputs": [],
      "source": [
        "# re-load the model to remove any previous PEFT tuners\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, \n",
        "    device_map={\"\": 0}, \n",
        "    low_cpu_mem_usage=True, \n",
        "    offload_state_dict=True,\n",
        "    load_in_4bit=True, \n",
        "    torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
        ")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "MJ_hq4fwyPVR"
      },
      "outputs": [],
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    \"\"\"Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer\"\"\"\n",
        "    def __init__(self, module: nn.Linear, rank: int):\n",
        "        super().__init__()\n",
        "        self.module = module  # pre-trained (frozen) linear layer\n",
        "        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n",
        "        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n",
        "        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Apply self.module and LoRA adapter, return the sum (self.module outputs + adapter outputs)\n",
        "        original_output = self.module(input)\n",
        "        lora_output = input @ self.adapter_A @ self.adapter_B\n",
        "\n",
        "        return original_output + lora_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTzOs65JydcS",
        "outputId": "e07177c9-2f2b-432a-8a97-9e507df166bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# test your implementation\n",
        "test_linear = nn.Linear(128, 128)\n",
        "test_linear.weight.data[...] = torch.eye(128)\n",
        "test_adapter = LoRALayer(test_linear, rank=8)\n",
        "\n",
        "assert torch.allclose(test_adapter(torch.ones(1, 1, 128)), test_linear.bias + 1), \"please check your forward pass\"\n",
        "\n",
        "test_adapter.adapter_A.data[...] = torch.linspace(0.1, -0.5, 128 * 8).view(128, 8)\n",
        "test_adapter.adapter_B.data[...] = torch.linspace(0.5, -0.1, 128 * 8).view(8, 128)\n",
        "test_linear.bias.data[...] = torch.linspace(1., -1., 128)\n",
        "\n",
        "dummy_loss = F.mse_loss(test_adapter(torch.ones(1, 128) / 128).squeeze(), torch.linspace(-1, 1, 128))\n",
        "assert torch.allclose(dummy_loss, torch.tensor(1.3711389), rtol=0, atol=1e-4)\n",
        "dummy_loss.backward()\n",
        "assert all(w.grad is not None for w in [test_adapter.adapter_A, test_adapter.adapter_B]), \"some adapter weights have no grad\"\n",
        "assert torch.allclose(test_adapter.adapter_A.grad.sum(), torch.tensor(-0.60158), rtol=0, atol=1e-4), \"bad grad w.r.t. A\"\n",
        "assert torch.allclose(test_adapter.adapter_B.grad.sum(), torch.tensor(0.9931), rtol=0, atol=1e-4), \"bad grad w.r.t. B\"\n",
        "# note: bad grad means that your code is different from LoRA paper OR that your code is not autograd-friendly (e.g. no_grad)\n",
        "del dummy_loss, test_linear, test_adapter\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tajVTsvLulB6"
      },
      "source": [
        "### Apply LoRA to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "davyUVEwulB6"
      },
      "outputs": [],
      "source": [
        "lora_rank = 8\n",
        "\n",
        "for name, module in model.model.layers.named_modules():\n",
        "    if 'LlamaDecoderLayer' in repr(type(module)):\n",
        "        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank).to(device)\n",
        "        module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank).to(device)\n",
        "        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(device)\n",
        "\n",
        "assert sum(isinstance(module, LoRALayer) for module in model.modules()) == 96  # for Llama-7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWzfvc0EulB6",
        "outputId": "b432afe7-08b9-4cb2-c6ac-01f85352a689"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_5020/3590305689.py:3: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float32):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grad check successful, well done!\n"
          ]
        }
      ],
      "source": [
        "batch = tokenizer(\"This model wants to share its greatest secret:\", return_tensors='pt', return_token_type_ids=False)\n",
        "# test a single training step, make sure we get meaningful gradients\n",
        "with torch.cuda.amp.autocast(dtype=torch.float32):\n",
        "    out = model.forward(**batch)\n",
        "    (out.logits.norm() / 100).backward()\n",
        "\n",
        "for i, module in enumerate(model.modules()):\n",
        "    if isinstance(module, LoRALayer):\n",
        "        assert module.adapter_B.grad is not None\n",
        "        assert module.adapter_B.grad.norm().item() > 0\n",
        "\n",
        "model.zero_grad(set_to_none=True)\n",
        "print(\"Grad check successful, well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQUlqoEAulB8"
      },
      "source": [
        "### train the model \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "from IPython.display import HTML, display\n",
        "import os\n",
        "\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de912a84c811487390add61add1b7512",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Загрузка модели\n",
        "model_name = 'Enoch/llama-7b-hf'\n",
        "\n",
        "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map={\"\": 0},\n",
        "    low_cpu_mem_usage=True,\n",
        "    offload_state_dict=True,\n",
        "    load_in_4bit=True,\n",
        "    torch_dtype=torch.float32,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Заморозка\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# мой LoRALayer\n",
        "class LoRALayer(nn.Module):\n",
        "    def __init__(self, module: nn.Linear, rank: int):\n",
        "        super().__init__()\n",
        "        self.module = module  \n",
        "        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n",
        "        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n",
        "        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n",
        "\n",
        "    def forward(self, input):\n",
        "        original_output = self.module(input)\n",
        "        lora_output = input @ self.adapter_A @ self.adapter_B\n",
        "\n",
        "        return original_output + lora_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Обучаемые параметры: 96\n"
          ]
        }
      ],
      "source": [
        "lora_rank = 8\n",
        "for name, module in model.model.layers.named_modules():\n",
        "    if 'LlamaDecoderLayer' in repr(type(module)):\n",
        "        try:\n",
        "            if hasattr(module, 'mlp') and isinstance(module.mlp.gate_proj, nn.Linear):\n",
        "                module.mlp.gate_proj = LoRALayer(module.mlp.gate_proj, rank=lora_rank).to(device)\n",
        "                module.mlp.up_proj = LoRALayer(module.mlp.up_proj, rank=lora_rank).to(device)\n",
        "                module.mlp.down_proj = LoRALayer(module.mlp.down_proj, rank=lora_rank).to(device)\n",
        "        except AttributeError:\n",
        "             continue\n",
        "\n",
        "total_lora_layers = sum(isinstance(module, LoRALayer) for module in model.modules())\n",
        "print(f\"Обучаемые параметры: {total_lora_layers:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "496910f98c4b4efeb3cb7dd2f71551e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/54 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Общее количество частей: 417\n"
          ]
        }
      ],
      "source": [
        "# Подготовка датасета\n",
        "block_size = 256\n",
        "\n",
        "dataset = load_dataset(\"codeparrot/codeparrot-clean\", split=\"train\", streaming=True)\n",
        "dataset = dataset.shuffle(seed=42, buffer_size=1000).take(650)\n",
        "\n",
        "all_text = \" \".join(item['content'][:512] for item in dataset if isinstance(item['content'], str))\n",
        "tokenized = tokenizer(all_text, truncation=False, add_special_tokens=False)\n",
        "input_ids = tokenized['input_ids']\n",
        "\n",
        "chunks = [\n",
        "    input_ids[i : i + block_size]\n",
        "    for i in range(0, len(input_ids) - block_size, block_size)\n",
        "    if len(input_ids[i : i + block_size]) == block_size\n",
        "]\n",
        "\n",
        "print(f\"Общее количество частей: {len(chunks)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ChunkDataset(Dataset):\n",
        "    def __init__(self, chunks):\n",
        "        self.chunks = chunks\n",
        "    def __len__(self):\n",
        "        return len(self.chunks)\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.chunks[idx], dtype=torch.long)\n",
        "\n",
        "train_dataset = ChunkDataset(chunks)\n",
        "data_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompts = ['import', 'from', 'while', 'try', 'if', 'for', 'torch', 'def']\n",
        "\n",
        "def generate_samples(model, prompts, max_new_tokens=80):\n",
        "    \"\"\"\n",
        "    Функция генерации текста\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    results = []\n",
        "    with torch.no_grad():\n",
        "        for p in prompts:\n",
        "            inputs = tokenizer(p, return_tensors=\"pt\").to(device)\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,\n",
        "                top_p=0.95,\n",
        "                temperature=0.7,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "            gen = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            results.append(gen)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Генерация до обучения...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
          ]
        }
      ],
      "source": [
        "print(\"Генерация до обучения...\")\n",
        "before_samples = generate_samples(model, prompts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cff88aa918b347e6aaef9da1374576c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training LoRA on Python code:   0%|          | 0/500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed!\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "model.train()\n",
        "\n",
        "data_iter = iter(data_loader)\n",
        "steps = 0\n",
        "max_steps = 500\n",
        "grad_accum = 4\n",
        "\n",
        "pbar = tqdm(total=max_steps, desc=\"Training LoRA on Python code\")\n",
        "\n",
        "while steps < max_steps:\n",
        "    optimizer.zero_grad()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for _ in range(grad_accum):\n",
        "        try:\n",
        "            batch = next(data_iter).to(device)\n",
        "        except StopIteration:\n",
        "            data_iter = iter(data_loader)\n",
        "            batch = next(data_iter).to(device)\n",
        "\n",
        "        if batch.shape[1] > block_size:\n",
        "            batch = batch[:, :block_size]\n",
        "\n",
        "        outputs = model(input_ids=batch, labels=batch)\n",
        "        loss = outputs.loss / grad_accum\n",
        "        loss.backward()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    optimizer.step()\n",
        "    steps += 1\n",
        "    pbar.set_postfix({'loss': total_loss})\n",
        "    pbar.update(1)\n",
        "\n",
        "pbar.close()\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Генерация после обучения...\n"
          ]
        }
      ],
      "source": [
        "model.config.use_cache = True\n",
        "\n",
        "print(\"Генерация после обучения...\")\n",
        "after_samples = generate_samples(model, prompts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SSucUeB4ulB9",
        "outputId": "88f008b5-e68b-4949-d695-4d0de17cdd5c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table style=\"border:1px solid black\" >\n",
              "  <tr>\n",
              "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
              "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
              "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`import`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">math\n",
              "\n",
              "from . import Common, Datatype\n",
              "\n",
              "\n",
              "class Value(Common.Value):\n",
              "    def __init__(self, name, value, source):\n",
              "        Common.Value.__init__(self, name, value, source)\n",
              "\n",
              "    def __repr__(self):\n",
              "       ...</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">re\n",
              "import urllib2\n",
              "import urllib2 import urllib2\n",
              "import urllib2\n",
              "import urllib2\n",
              "import urllib2\n",
              "import urllib2\n",
              "import urllib2\n",
              "import urllib2\n",
              "import urllib2\n",
              "import urllib2\n",
              "import urllib2\n",
              "import urllib2\n",
              "im...</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`from`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">oslo_logs import context\n",
              "from oslo_log import importutils\n",
              "from oslo_log import log as logging\n",
              "from oslo_log import resources\n",
              "from oslo_log.strategy import Strategy\n",
              "\n",
              "import six\n",
              "\n",
              "\n",
              "class MultiLog(resourc...</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">django.conf import settings\n",
              "from django.db import migrations, models\n",
              "import django.db.models.deletion\n",
              "\n",
              "\n",
              "class Migration(migrations.Migration):\n",
              "\n",
              "    dependencies = [\n",
              "        ('accounting', '0012_remove...</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`while`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">the second is a list of 200+ names from the 1820 US Federal Census.\n",
              "I am not a genealogist, nor am I related to any of the names in the two lists.\n",
              "The first list is a list of names from the 1820 US Fe...</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">(1)\n",
              "import time\n",
              "import datetime\n",
              "\n",
              "now = datetime.datetime.now()\n",
              "print now\n",
              "\n",
              "while 1:\n",
              "    print 'Checking if it is now', now\n",
              "    time.sleep(300)\n",
              "    now = datetime.datetime.now()\n",
              "    print now\n",
              "\\end{code}...</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`try`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">again. if you are having issues, please contact support@stupeflix.com with your account name and a link to your video.</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">:\n",
              "    from _ast import *\n",
              "except ImportError:\n",
              "    pass\n",
              "try:\n",
              "    from _ast import Tuple\n",
              "except ImportError:\n",
              "    pass\n",
              "\n",
              "class Node(object):\n",
              "    def __init__(self, value, line=None, col=None, expanded=Fals...</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`if`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">(var_is_null())\n",
              "  {\n",
              "    return;\n",
              "  }\n",
              "\n",
              "  var_ = var_;\n",
              "  if (var_.is_null())\n",
              "    {\n",
              "      return;\n",
              "    }\n",
              "\n",
              "  if (var_.get_type() == Var::VARCHAR)\n",
              "    {\n",
              "      var_.to_string(str);\n",
              "    }</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">(typeof require === 'function') {\n",
              "    var Rx = require('../../../../../src/core/util/core').Rx;\n",
              "}\n",
              "\n",
              "describe('Observable#take', function () {\n",
              "\n",
              "    it('should take the first values from the observable s...</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`for`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">the 2019-2020 school year\n",
              "SPECIAL AWARD TIME!\n",
              "All students should be in the bleachers by 8:45 AM.\n",
              "The Pledge of Allegiance will be recited at 8:50 AM.\n",
              "The awards will begin at 9:00 AM.\n",
              "2019</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">PYTHON_ARCHIVES\n",
              "The Python Software Foundation (\"the Foundation\") is a 501(c)(3) non-profit corporation incorporated in the Commonwealth of Massachusetts. The Foundation is the legal home of the Pytho...</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`torch`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">-bearer - a person who passes on a tradition, custom, or duty\n",
              "To be a torchbearer for the Gospel is to be a torchbearer for the kingdom of God. The Gospel is the Good News that the kingdom of God is h...</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">.frontend.Terminal \n",
              "-- Copyright (c) 2013-2017 The Chancellor Master and Fellows of the University of Cambridge\n",
              "--\n",
              "-- This file is part of torch.\n",
              "--\n",
              "-- Torch is free software: you can redistribute it ...</pre></td>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`def`</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">ixion 2017-07-03 03:35:55 UTC #1\n",
              "I’ve been using the 360cam for a while now and have always been disappointed with the video quality. I’m using the 360cam app on a Samsung galaxy s7. I’ve noticed that...</pre></td>\n",
              "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">get_current_user():\n",
              "    \"\"\"\n",
              "    Returns the current user from the session.\n",
              "\n",
              "    If there is no session, or the session is empty, it will return None.\n",
              "\n",
              "    :returns: \n",
              "        None or the string id of t...</pre></td>\n",
              "  </tr>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# This template helps to compare generated code samples in pretty table form\n",
        "# feel free to present your work in other forms\n",
        "\n",
        "table_template = \"\"\"<table style=\"border:1px solid black\" >\n",
        "  <tr>\n",
        "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n",
        "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n",
        "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n",
        "  </tr>\n",
        "{}\n",
        "</table>\"\"\"\n",
        "\n",
        "row_template = '''  <tr>\n",
        "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`{}`</pre></td>\n",
        "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
        "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n",
        "  </tr>'''\n",
        "\n",
        "rows = []\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "    before = before_samples[i].replace(prompt, \"\", 1).strip()\n",
        "    after = after_samples[i].replace(prompt, \"\", 1).strip()\n",
        "    rows.append(row_template.format(\n",
        "        prompt,\n",
        "        before[:200] + (\"...\" if len(before) > 200 else \"\"),\n",
        "        after[:200] + (\"...\" if len(after) > 200 else \"\")\n",
        "    ))\n",
        "\n",
        "display(HTML(table_template.format('\\n'.join(rows))))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09b07105e4f54d2bb5e6cc1c1f1a9c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "11019f88b11f4be39a22aada133b0a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25e1f3d72230485c9b84cae4f685a69a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bd4b6acd8004c1e98c064708108938e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31c4ef0dab98410d88891a2c27fdb5c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32d98d7256654cd58730286d1e93c6fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3321598939fd4d76957155f58097fadd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31c4ef0dab98410d88891a2c27fdb5c1",
            "placeholder": "​",
            "style": "IPY_MODEL_54b23e64bbc444b4abc3f25832e3677d",
            "value": " 32/32 [00:00&lt;00:00, 325.58 examples/s]"
          }
        },
        "530d66e4732b4d5486165654415bd2dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54b23e64bbc444b4abc3f25832e3677d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d40abb9d0514fccbea444fc6d2326d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e210039d1ac4c7f9ef85f65cd5b0371": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6423f3812d6646c6a416765f7710ebff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d40abb9d0514fccbea444fc6d2326d3",
            "placeholder": "​",
            "style": "IPY_MODEL_93955e8872df4a85ab7db725856546d2",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6aad5b046def4a7db1048434e874b5d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71f8dcf1e6f2421d8d0e0aa57e2880ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aea6cd9a18b4dd9b40bbe65fb0b9069": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b45cca01b224a598961f4b281c41418": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e210039d1ac4c7f9ef85f65cd5b0371",
            "placeholder": "​",
            "style": "IPY_MODEL_11019f88b11f4be39a22aada133b0a6e",
            "value": " 33/33 [01:42&lt;00:00,  3.57s/it]"
          }
        },
        "895c44b30fd24b8b9bbedc631a7934ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ad2b69a25304e5f903f2fd43b538340",
            "placeholder": "​",
            "style": "IPY_MODEL_7aea6cd9a18b4dd9b40bbe65fb0b9069",
            "value": "Map: 100%"
          }
        },
        "8aaa22971b3e416eae8394ef3b3b3f0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dba48e929a2e43ec8e4bb3d4b32475ca",
            "placeholder": "​",
            "style": "IPY_MODEL_530d66e4732b4d5486165654415bd2dc",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8ad2b69a25304e5f903f2fd43b538340": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8afd16e7ce954ef3a8904bd5ab58c58b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6423f3812d6646c6a416765f7710ebff",
              "IPY_MODEL_cf077c6f0f2347faad3ab340608017a4",
              "IPY_MODEL_7b45cca01b224a598961f4b281c41418"
            ],
            "layout": "IPY_MODEL_71f8dcf1e6f2421d8d0e0aa57e2880ac"
          }
        },
        "923869a5864c4d3d80fb76c99fff24e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92f3ee2ed68c4defbed2fe9bef0f20b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f72b2d490b04440b800ac3c8ab05e625",
            "max": 32,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6ddf10ea9ef499f917c81fde3e63cd2",
            "value": 32
          }
        },
        "93955e8872df4a85ab7db725856546d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a868f8a190f74df2a99a50e2ca9a7cb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_895c44b30fd24b8b9bbedc631a7934ec",
              "IPY_MODEL_92f3ee2ed68c4defbed2fe9bef0f20b2",
              "IPY_MODEL_3321598939fd4d76957155f58097fadd"
            ],
            "layout": "IPY_MODEL_f0fe9da3411840ef8d7eff80883cb8e9"
          }
        },
        "b87c54c3bed847ab933eae8175359169": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8aaa22971b3e416eae8394ef3b3b3f0f",
              "IPY_MODEL_e9c4ba0d262c4b76baa00532e209b92f",
              "IPY_MODEL_e6f9064e6ec545debe2e90163f4c712c"
            ],
            "layout": "IPY_MODEL_6aad5b046def4a7db1048434e874b5d5"
          }
        },
        "c6ddf10ea9ef499f917c81fde3e63cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6ed8fa489d54cf7bd83c8ec1890242b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf077c6f0f2347faad3ab340608017a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6ed8fa489d54cf7bd83c8ec1890242b",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32d98d7256654cd58730286d1e93c6fe",
            "value": 33
          }
        },
        "dba48e929a2e43ec8e4bb3d4b32475ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6f9064e6ec545debe2e90163f4c712c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_923869a5864c4d3d80fb76c99fff24e2",
            "placeholder": "​",
            "style": "IPY_MODEL_25e1f3d72230485c9b84cae4f685a69a",
            "value": " 33/33 [01:49&lt;00:00,  3.12s/it]"
          }
        },
        "e9c4ba0d262c4b76baa00532e209b92f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bd4b6acd8004c1e98c064708108938e",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09b07105e4f54d2bb5e6cc1c1f1a9c8e",
            "value": 33
          }
        },
        "f0fe9da3411840ef8d7eff80883cb8e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f72b2d490b04440b800ac3c8ab05e625": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
